{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"a7vu56UAIGD0"},"outputs":[],"source":["from google.colab import userdata\n","from huggingface_hub import login\n","import os, gc\n","\n","hf_token = userdata.get('HF_Read_Token')\n","os.environ[\"HF_TOKEN\"] = hf_token\n","login(token=hf_token)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-LO0E8Jauo2"},"outputs":[],"source":["%pip install --quiet --upgrade transformers accelerate torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xeprY28BeK1d"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, pipeline"]},{"cell_type":"code","source":["def clear_cache():\n","  gc.collect()\n","  torch.cuda.empty_cache()\n","  print(\"\\n\\nCleared GPU Cache\")"],"metadata":{"id":"-OttxZJz-u6w"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tS4a5JVa8Md"},"outputs":[],"source":["# model_name: str = \"Salesforce/codegen-350M-mono\"\n","# model_name: str = \"bigcode/santacoder\"\n","model_name: str = \"microsoft/Phi-3.5-mini-instruct\"\n","# nlp_model_name: str = \"google/flan-t5-large\"\n","# nlp_model_name: str = \"google/flan-t5-base\"\n","\n","is_cuda: bool = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if is_cuda else \"cpu\")\n","# nlp_device = torch.device(\"cpu\")\n","\n","print(\"Using device:\", device)\n","print(\"Using model:\", model_name)\n","# print(\"Using NLP model:\", nlp_model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wT5-ecLogs8"},"outputs":[],"source":["try:\n","  del tokenizer\n","  del model\n","  # del nlp_tokenizer\n","  # del nlp_model\n","  clear_cache()\n","except:\n","  pass\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n","\n","print(\"Initialized Tokenizer\")\n","\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    token=hf_token,\n","    device_map=\"auto\",\n","    dtype=torch.float16\n",")\n","\n","print(\"Pretrained the Model\")\n","\n","# nlp_tokenizer = AutoTokenizer.from_pretrained(nlp_model_name, token=hf_token)\n","\n","# print(\"Initialized NLP Tokenizer\")\n","\n","# nlp_model = AutoModelForSeq2SeqLM.from_pretrained(\n","#       nlp_model_name,\n","#       token=hf_token,\n","#       device_map=nlp_device,\n","#       dtype=torch.float16\n","# )\n","\n","# print(\"Pretrained the NLP Model\")\n","\n","clear_cache()"]},{"cell_type":"markdown","metadata":{"id":"zW04_3O-JNvL"},"source":["**Code Starts Here**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSyBNiOkpSVs"},"outputs":[],"source":["language: str = \"Python\" # \"C\", \"Python\"\n","prompt: str = f\"You're a {language} Programmer who can write code in {language} language\\n\\\n","Generate a perfectly working Standard {language} code for Bubble-Sort Algorithm for sorting a provided int array of size `n`.\\n\\\n","Do not include code from any other language. Only return {language} code.\\n\\\n","Sure, here is the code:\\n```{language.lower()}\\n\"\n","\n","tokenized_prompt = tokenizer(\n","    prompt,\n","    return_tensors=\"pt\"\n",").to(device)\n","\n","output_tokens = model.generate(\n","        **tokenized_prompt,\n","        do_sample=False,\n","        # temperature=0.7,\n","        # top_p=0.95,\n","        pad_token_id=tokenizer.eos_token_id,\n","        eos_token_id=tokenizer.eos_token_id,\n","        max_new_tokens=100 if language.capitalize() == \"Python\" else 250 if language.capitalize() == \"C\" else 150,\n","        min_new_tokens=10\n",")\n","\n","outputs = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n","print(outputs)\n","\n","outputs = outputs[len(prompt):]\n","# print(\"\\n\\nRequired Code:\\n\", outputs, \"\\n\\n\")\n","\n","triple_backtick_index: int = outputs.find(\"```\")\n","if triple_backtick_index >= 0:\n","  print(f\"\\n\\n\\nExpected code upto triple_backtick_index={triple_backtick_index}:\\n\", outputs[:triple_backtick_index])\n","print(f\"\\nThis was the length of the prompt={len(prompt)}\")\n","\n","del outputs\n","del output_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2qCX835_5FT"},"outputs":[],"source":["# def generate_prompt(prompt: str, max_tokens: int=50, min_tokens: int=10) -> str:\n","#   nlp_prompt = f\"\"\"### Instruction:\n","# You are a prompt refiner.\n","# Rewrite the following prompt into a clear, strict instruction for a coding agent.\n","# Always include:\n","# - \"You are a {{language}} programmer\"\n","# - \"Only return code in that language\"\n","# Do not add explanations or commentary. Only return the improved prompt.\n","\n","# ### Prompt:\n","# {prompt}\n","\n","# ### Response:\"\"\"\n","#   tokenized_nlp_prompt = nlp_tokenizer(\n","#       nlp_prompt,\n","#       return_tensors=\"pt\",\n","#       truncation=True,\n","#       padding=True\n","#   ).to(nlp_device)\n","\n","#   nlp_output_tokens = nlp_model.generate(\n","#           **tokenized_nlp_prompt,\n","#           do_sample=True,\n","#           temperature=0.7,\n","#           top_p=0.9,\n","#           pad_token_id=nlp_tokenizer.eos_token_id,\n","#           eos_token_id=nlp_tokenizer.eos_token_id,\n","#           max_new_tokens=max_tokens,\n","#           min_new_tokens=min_tokens if min_tokens > 0 else 10\n","#   )\n","\n","#   # print(\"Prompt passed=\\n\", nlp_prompt)\n","\n","#   nlp_output = nlp_tokenizer.decode(nlp_output_tokens[0], skip_special_tokens=True)\n","#   # print(\"\\n\\nRefined prompt=\\n\", nlp_output)\n","\n","#   return nlp_output"]},{"cell_type":"code","source":["def generate_code(prompt: str, max_tokens: int=200, min_tokens: int=10) -> str:\n","  tokenized_prompt = tokenizer(\n","      prompt,\n","      return_tensors=\"pt\"\n","  ).to(device)\n","\n","  output_tokens = model.generate(\n","          **tokenized_prompt,\n","          do_sample=False,\n","          # temperature=0.7,\n","          # top_p=0.95,\n","          pad_token_id=tokenizer.eos_token_id,\n","          eos_token_id=tokenizer.eos_token_id,\n","          max_new_tokens=max_tokens,\n","          min_new_tokens=min_tokens\n","  )\n","\n","  outputs = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n","  # print(outputs)\n","\n","  outputs = outputs[len(prompt):]\n","  # print(\"\\n\\nRequired Code:\\n\", outputs, \"\\n\\n\")\n","\n","  triple_backtick_index: int = outputs.find(\"```\")\n","  if triple_backtick_index >= 0:\n","    outputs = outputs[:triple_backtick_index]\n","    # print(f\"\\n\\n\\nExpected code upto triple_backtick_index={triple_backtick_index}:\\n\", outputs[:triple_backtick_index])\n","  # print(f\"\\nThis was the length of the prompt={len(prompt)}\")\n","\n","  return outputs"],"metadata":{"id":"V-U1vYJ8BNOA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["python_max_tokens: int = 200\n","c_max_tokens: int = 300\n","\n","for language in [\"C\", \"Python\"]:\n","  prompt: str = f\"Write a function in {language} programming language for Bubble-Sort Algorithm\"\n","  print(\"prompt=\\n\", prompt, end=\"\\n\\n\")\n","\n","  # nlp_prompt: str = generate_prompt(prompt)\n","  # print(\"nlp_prompt=\\n\", nlp_prompt, end=\"\\n\\n\")\n","\n","  # nlp_prompted_code: str = generate_code(prompt=nlp_prompt, max_tokens=python_max_tokens if language.capitalize() == \"Python\" else c_max_tokens if language.upper() == \"C\" else 150)\n","  # print(\"\\nnlp_prompt_code=\\n\", nlp_prompted_code)\n","\n","  general_code: str = generate_code(prompt=prompt, max_tokens=python_max_tokens if language.capitalize() == \"Python\" else c_max_tokens if language.upper() == \"C\" else 150)\n","  print(\"\\ngeneral_code=\\n\", general_code)"],"metadata":{"id":"ZDzgVKTCJEXe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","  del python_max_tokens\n","  del c_max_tokens\n","  # del nlp_prompt\n","  # del nlp_prompted_code\n","  del general_code\n","except:\n","  pass\n","\n","clear_cache()"],"metadata":{"id":"MzLyx2jm5QZA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DoC5LIvhUJ5y"},"outputs":[],"source":["prompt: str = \"You're a Javascript Programmer who can write code in Javascript language\\n\\\n","    Generate a perfectly working Vanilla-Javascript code a Program for traversing through all the HTML DOM elements and changing their CSS-style background to red.\\n\\\n","    Do not include code from any other language. Only return Javascript code.\\\n","    Sure, here is the code:\\n```javascript\\n\"\n","\n","prompt: str = \"Write a program in Javascript programming language to traverse through all HTML DOM elements and change their CSS-style background to red.\\n```\\n\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LOQH0CVDKrPR"},"outputs":[],"source":["print(\"prompt=\\n\", prompt, end=\"\\n\\n\")\n","\n","# nlp_prompt: str = generate_prompt(prompt)\n","# print(\"nlp_prompt=\\n\", nlp_prompt, end=\"\\n\\n\")\n","\n","# nlp_prompted_code: str = generate_code(prompt=nlp_prompt, max_tokens=100)\n","# print(\"\\nnlp_prompt_code=\\n\", nlp_prompted_code)\n","\n","general_code: str = generate_code(prompt=prompt, max_tokens=100)\n","print(\"\\ngeneral_code=\\n\", general_code)"]},{"cell_type":"code","source":["# del nlp_prompt\n","# del nlp_prompted_code\n","del general_code\n","clear_cache()"],"metadata":{"id":"l5lHpksnE-Gd"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1ktIFmJdwGmxXIwbZ56mBQOssF7pYxSTG","timestamp":1758288726088}],"authorship_tag":"ABX9TyNES+qBQj1IZVnAJEDcG+RB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}